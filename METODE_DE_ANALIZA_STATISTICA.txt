METODE DE ANALIZÄ‚ STATISTICÄ‚

CCA

import numpy as np
from sklearn.cross_decomposition import CCA

x = np.ndarray() # standardized
y = np.ndarray() # standardized

p = x.shape[1]
q = y.shape[1]
m = min(p, q)
cca = CCA(n_components=m)
z, u = cca.fit_transform(x, y)
rxz = np.corrcoef(x, z[:, :m], rowvar=False)[:p, p:]
ryu = np.corrcoef(y, u[:, :m], rowvar=False)[:q, q:]
r = []
for i in range(m):
    r.append(np.corrcoef(z[:, i], u[:, i], rowvar=False)[0, 1])
 
--CCA (Canonical Correlation Analysis) este o metodÄƒ statisticÄƒ utilizatÄƒ pentru a gÄƒsi relaÈ›iile liniare dintre douÄƒ seturi de variabile.

ðŸ“Œ Ce face concret codul?
-StandardizeazÄƒ datele x  È™i y (presupunÃ¢nd cÄƒ sunt deja normalizate).
-CalculeazÄƒ componentele canonice z  È™i u,  care maximizeazÄƒ corelaÈ›ia dintre variabilele din x  È™i y.
-DeterminÄƒ corelaÈ›iile dintre variabilele originale È™i componentele rezultate.
-CalculeazÄƒ corelaÈ›iile canonice r,  care aratÄƒ cÃ¢t de bine sunt corelate componentele extrase Ã®ntre cele douÄƒ seturi de variabile.
ðŸ“Œ Rolul CCA
CCA este folosit pentru a identifica È™i interpreta relaÈ›iile dintre douÄƒ seturi de date diferite, dar corelate (ex: relaÈ›ia dintre performanÈ›ele academice È™i testele psihologice).

EFA

import numpy as np
from factor_analyzer import FactorAnalyzer, calculate_kmo

x = np.ndarray() # standardized

kmo = calculate_kmo(x) # kmo[1] needs to be > 0.6

efa = FactorAnalyzer(n_factors=x.shape[1] - 1) # n_factors needs to be no. columns - 1
scores = efa.fit_transform(x)
factorLoadings = efa.loadings_
eigenvalues = efa.get_eigenvalues()
communalities = efa.get_communalities()
specificFactors = efa.get_uniquenesses()

--EFA (Exploratory Factor Analysis) este o metodÄƒ statisticÄƒ utilizatÄƒ pentru a descoperi structuri latente (factori) Ã®ntr-un set de variabile È™i pentru a reduce dimensiunea datelor.

ðŸ“Œ Ce face concret codul?
-CalculeazÄƒ KMO (Kaiser-Meyer-Olkin), o mÄƒsurÄƒ care verificÄƒ dacÄƒ datele sunt potrivite pentru analiza factorialÄƒ (valoarea trebuie sÄƒ fie > 0.6).
-AplicÄƒ EFA folosind un numÄƒr de factori = numÄƒrul de coloane - 1.
-ObÈ›ine factorii latenti (scores) È™i matricea de Ã®ncÄƒrcÄƒturi factoriale (factorLoadings).
-CalculeazÄƒ valorile proprii (eigenvalues) â€“ indicÄƒ importanÈ›a fiecÄƒrui factor.
-DeterminÄƒ comunalitÄƒÈ›ile (proporÈ›ia varianÈ›ei explicate de factori pentru fiecare variabilÄƒ).
-CalculeazÄƒ unicitatea fiecÄƒrei variabile (partea din variabilÄƒ care nu este explicatÄƒ de factori).
ðŸ“Œ Rolul EFA
EFA este folosit pentru a identifica structuri ascunse Ã®n date, a reduce dimensionalitatea È™i a gÄƒsi relaÈ›ii Ã®ntre variabile. Este aplicat Ã®n psihologie, sociologie, economie, etc.

HCA

import matplotlib.pyplot as plt
import numpy as np
from scipy.cluster.hierarchy import fcluster, linkage
from sklearn.cluster import KMeans

x = np.ndarray() # standardized

def threshold(h: np.ndarray):
    n = h.shape[0] # no of junctions
    dist_1 = h[1:n, 2]
    dist_2 = h[0:n - 1, 2]
    diff = dist_1 - dist_2
    j = np.argmax(diff) # junction with max. diff
    t = (h[j, 2] + h[j + 1, 2]) / 2 # threshold
    return t, j, n

def clusters(h: np.ndarray, k):
    cat = fcluster(h, k, criterion='maxclust')
    return ['C' + str(i) for i in cat]

HC = linkage(x, method='ward') # the method is given in the requirements
t, j, n = threshold(HC)

# determine the clusters belonging to the maximum stability partition
k = n - j
labels = clusters(HC, k) # add this to the original DataFrame

# KMeans
C = np.ndarray() # principal components
kmeans = KMeans(n_clusters=5, n_init=10) # n_clusters is given in the requirements
kmeans_labels = kmeans.fit_predict(C)
plt.scatter(C[:, 0], C[:, 1], c=kmeans_labels, cmap='viridis')

--HCA (Hierarchical Cluster Analysis) este o metodÄƒ de grupare ierarhicÄƒ care construieÈ™te un dendrogramÄƒ È™i determinÄƒ clusterele pe baza unei mÄƒsuri de similaritate Ã®ntre observaÈ›ii.

ðŸ“Œ Ce face concret codul?
-CalculeazÄƒ matricea de legÄƒturi cu metoda Ward (HC = linkage(x, method='ward')).
-DeterminÄƒ pragul optim pentru tÄƒierea dendrogramei (threshold(HC)).
-IdentificÄƒ cel mai mare salt Ã®n distanÈ›ele de fuziune.
-CalculeazÄƒ pragul t pentru stabilirea numÄƒrului optim de clustere.
-Atribuie fiecare punct unui cluster (clusters(HC, k)).
-AplicÄƒ K-Means pentru compararea rezultatelor clusteringului ierarhic cu K-Means.
ðŸ“Œ Rolul HCA
HCA este folosit pentru a identifica structuri de grupare Ã®n date, fiind util Ã®n clasificare È™i analizÄƒ exploratorie. Se aplicÄƒ Ã®n marketing (segmentare de clienÈ›i), procesare de imagini etc.

LDA

import pandas as pd
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split

x = pd.DataFrame() # DOES NOT need to be standardized
x_applied = pd.DataFrame() # DOES NOT need to be standardized

tinta = 'VULNERAB' # column specified in the requirements
variabile = list(x.columns.values[:-1]) # the other columns

x_train, x_test, y_train, y_test = train_test_split(x[variabile], x[tinta], train_size=0.4)
lda = LinearDiscriminantAnalysis()
lda.fit(x_train, y_train) # trains the model

scores = lda.transform(x_test)
prediction_test = lda.predict(x_test)
prediction_applied = lda.predict(x_applied)

--LDA (Linear Discriminant Analysis) este o metodÄƒ de reducere a dimensiunii È™i clasificare care maximizeazÄƒ separarea dintre clase Ã®ntr-un set de date etichetat.

ðŸ“Œ Ce face concret codul?
-ÃŽmparte datele Ã®n antrenare È™i testare (train_test_split).
-AntreneazÄƒ un model LDA (lda.fit(x_train, y_train)).
-TransformÄƒ datele pentru a obÈ›ine un nou spaÈ›iu cu dimensiuni reduse (lda.transform(x_test)).
-Face predicÈ›ii pentru test (lda.predict(x_test)) È™i pentru un nou set de date (lda.predict(x_applied)).
ðŸ“Œ Rolul LDA
LDA este utilizat pentru clasificare È™i reducerea dimensiunii, fiind folosit Ã®n recunoaÈ™tere facialÄƒ, bioinformaticÄƒ È™i analiza datelor economice.

PCA

import pandas as pd
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split

x = pd.DataFrame() # DOES NOT need to be standardized
x_applied = pd.DataFrame() # DOES NOT need to be standardized

tinta = 'VULNERAB' # column specified in the requirements
variabile = list(x.columns.values[:-1]) # the other columns

x_train, x_test, y_train, y_test = train_test_split(x[variabile], x[tinta], train_size=0.4)
lda = LinearDiscriminantAnalysis()
lda.fit(x_train, y_train) # trains the model

scores = lda.transform(x_test)
prediction_test = lda.predict(x_test)
prediction_applied = lda.predict(x_applied)

--PCA (Analiza Componentelor Principale) este o metodÄƒ de reducere a dimensiunii care transformÄƒ datele Ã®ntr-un nou sistem de coordonate, maximizÃ¢nd variaÈ›ia È™i eliminÃ¢nd redundanÈ›a.

ðŸ“Œ Ce face concret codul?
-AplicÄƒ PCA (pca.fit_transform(x)) â†’ obÈ›ine noile componente principale.
-CalculeazÄƒ varianÈ›a explicatÄƒ (pca.explained_variance_) â†’ mÄƒsoarÄƒ importanÈ›a fiecÄƒrei componente.
-DeterminÄƒ Ã®ncÄƒrcÄƒturile factorilor (rxc) â†’ corelaÈ›ia variabilelor originale cu noile componente.
-CalculeazÄƒ scorurile (scores = C / np.sqrt(alpha)) â†’ reprezintÄƒ observaÈ›iile Ã®n noul spaÈ›iu redus.
-MÄƒsoarÄƒ calitatea proiecÈ›iei (quality) È™i contribuÈ›ia fiecÄƒrei componente (contributions).
-ObÈ›ine comunalitÄƒÈ›ile (communalities) â†’ cÃ¢t din informaÈ›ia iniÈ›ialÄƒ este pÄƒstratÄƒ Ã®n noile componente.
-CalculeazÄƒ procentul varianÈ›ei explicate (pve = pca.explained_variance_ratio_) â†’ aratÄƒ cÃ¢t de bine explicÄƒ fiecare componentÄƒ variaÈ›ia totalÄƒ.
ðŸ“Œ Rolul PCA
PCA este folosit pentru reducerea dimensiunii datelor, eliminarea colinearitÄƒÈ›ii È™i vizualizarea relaÈ›iilor dintre variabile. Se aplicÄƒ Ã®n recunoaÈ™tere facialÄƒ, bioinformaticÄƒ, economie È™i machine learning.

GRAFICE

import matplotlib.pyplot as plt
import numpy as np
from scipy.cluster.hierarchy import dendrogram
from seaborn import heatmap, kdeplot


def correlogram(x):
    plt.figure(figsize=(15, 11))
    plt.title('Correlogram')
    heatmap(data=x, vmin=-1, vmax=1, cmap='bwr', annot=True)

def linePlot(alpha):
    plt.figure(figsize=(11, 8))
    plt.title('Line plot')
    Xindex = ['C' + str(k + 1) for k in range(len(alpha))]
    plt.plot(Xindex, alpha, 'bo-')
    plt.axhline(1, color='r')

def biplot(x, y):
    plt.figure(figsize=(7, 7))
    plt.title('Biplot CCA')
    plt.xlabel("x")
    plt.ylabel("y")
    plt.scatter(x[:, 0], x[:, 1], c='r', label='X')
    plt.scatter(y[:, 0], y[:, 1], c='b', label='Y')
    plt.legend()

def dendrogram(h, labels, threshold):
    plt.figure(figsize=(15, 8))
    plt.title('Clusters')
    dendrogram(h, labels=labels, leaf_rotation=30)
    plt.axhline(threshold, c='r')

def correlationCircle(data):
    plt.figure(figsize=(12, 12))
    plt.title('Correlation circle')
    T = [t for t in np.arange(0, np.pi*2, 0.01)]
    X = [np.cos(t) for t in T]
    Y = [np.sin(t) for t in T]
    plt.plot(X, Y)
    plt.axhline(0, c='g')
    plt.axvline(0, c='g')
    plt.scatter(data[:, 0], data[:, 1])

def kdeplot(scores):
    plt.figure(figseize=(12, 12))
    plt.title('Scores')
    kdeplot(scores, fill=True)

-- Correlogram (heatmap-ul cu corelaÈ›ii)

Scop: Analiza relaÈ›iilor dintre variabile printr-o matrice de corelaÈ›ii.
Utilizare: EFA (Exploratory Factor Analysis), PCA, analiza corelaÈ›iilor dintre variabile.
Interpretare: Valorile apropiate de 1 sau -1 indicÄƒ corelaÈ›ii puternice, 0 indicÄƒ lipsa corelaÈ›iei.

-- Line Plot (grafic de linie pentru alpha - varianÈ›Äƒ explicatÄƒ)

Scop: AratÄƒ varianÈ›a explicatÄƒ de fiecare componentÄƒ principalÄƒ.
Utilizare: PCA (Principal Component Analysis).
Interpretare: Componentele peste linia roÈ™ie (varianÈ›Äƒ >1) sunt relevante, restul pot fi ignorate.

-- Biplot (pentru CCA - Canonical Correlation Analysis)

Scop: VizualizeazÄƒ relaÈ›ia dintre seturile de variabile X È™i Y.
Utilizare: CCA (Canonical Correlation Analysis).
Interpretare: Punctele roÈ™ii È™i albastre aratÄƒ proiecÈ›ia variabilelor Ã®n noul spaÈ›iu.

-- Dendrogram (ierarhizare Ã®n clustering)

Scop: AfiÈ™eazÄƒ structura clusterelor È™i relaÈ›iile dintre ele.
Utilizare: HCA (Hierarchical Cluster Analysis).
Interpretare: Linia roÈ™ie indicÄƒ pragul de tÄƒiere pentru formarea clusterelor.

-- Correlation Circle (cercul corelaÈ›iilor Ã®n PCA/FA)

Scop: AratÄƒ relaÈ›ia dintre variabile È™i componentele principale.
Utilizare: PCA È™i EFA.
Interpretare: Variabilele apropiate Ã®ntre ele sunt corelate, iar cele mai Ã®ndepÄƒrtate de centru sunt bine reprezentate de acea componentÄƒ.

-- KDE Plot (Kernel Density Estimation pentru distribuÈ›ia scorurilor)

Scop: Analiza distribuÈ›iei scorurilor.
Utilizare: LDA, PCA, scorurile factorilor Ã®n FA.
Interpretare: AratÄƒ unde sunt concentrate cele mai multe observaÈ›ii.